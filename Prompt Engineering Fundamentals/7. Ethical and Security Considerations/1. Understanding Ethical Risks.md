<h1>Understanding Ethical Risks</h1>
<p>
	When designing prompts for large language models, it is important to recognize that outputs can reflect societal biases, stereotypes, or unintended misinformation. Language models learn from vast datasets, and the content they generate may inadvertently perpetuate these biases.
</p>
<p>
	For QA engineers and SDETs, this means that prompts used to generate test data, documentation, or automated reports could include biased or inappropriate information if not carefully structured. Ethical prompt engineering requires being aware of these risks and actively working to mitigate them.
</p>
<p>
	<b>Example:</b>
	A naive prompt asking the model to “Generate sample names for user accounts” may produce names that reflect cultural or gender biases present in the training data. By explicitly guiding the model with constraints such as diversity and neutrality, you can reduce the risk of biased outputs.
</p>
<p>
	<b>Key guidance:</b>
	Always consider the impact of generated content, especially when it could influence decisions, workflows, or automated systems. Prompts should be framed to minimize bias and avoid propagating harmful assumptions.
</p>

<footer>
	{Image placeholder: Illustration showing biased output vs ethically constrained output from the same prompt}
</footer>